{
  "Resource": "Testing theory of mind in large language models and humans",
  "Agency": "Nature Human Behaviour",
  "Year": "2024",
  "Responsible": "Christine Kakalou",
  "Description": "Provides a psychologically grounded, human-benchmarked evaluation framework for assessing social cognition and interaction capabilities of LLMs, directly informing multidimensional AI assessment beyond technical performance.",
  "Type": "Article",
  "Link": "https://www.nature.com/articles/s41562-024-01882-z",
  "Dimension": "Social",
  "Indicator": [
    "False belief reasoning",
    "Irony comprehension"
  ],
  "Code Available": "https://osf.io/fwj6v",
  "Tool Available": ""
}